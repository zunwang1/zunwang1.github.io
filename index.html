<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zun Wang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data/unc_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zun Wang</name>
              </p>
              <p>I'm a first-year CS Ph.D. student at UNC Chapel Hill, advised by <a href="https://www.cs.unc.edu/~mbansal/">Prof. Mohit Bansal</a>. I was a Master of Machine Learning and Computer Vision student at the Australian National University advised by <a href="http://users.cecs.anu.edu.au/~sgould/">Prof. Stephen Gould</a>. In my master time, I also interned at the OpenGVLab, Shanghai AI Laboratory, led by <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Prof. Yu Qiao</a>.
                 Before that, I got my bachelor degree in applied mathematics from the University of Science and Technology of China.
              </p>
              <p style="text-align:center">
                <a href="mailto:zunwang@cs.unc.edu">Email</a> &nbsp/&nbsp
                <a href="data/ZunWang_cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://twitter.com/ZunWang919">twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=G-jPT9MAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="data/anu-transcript.pdf">Transcript</a> &nbsp/&nbsp -->
                <a href="https://github.com/wz0919/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zunwang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zunwang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <p style="margin-bottom: 5px;">My research goal is to build multimodal, generative, and embodied agents, with current interests in:</p>
                <ul style="margin-top: 0; padding-left: 20px;">
                    <li>Multimodal Understanding and Generation</li>
                    <li>Scalable Learning for Embodied Agents</li>
                    <li>Multimodal Data Generation and Curation</li>
                </ul>
                
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li>(2025-01) &nbsp  <a href="https://arxiv.org/pdf/2412.08467">Self-refining Data Flywheel</a> for high-quality VLN data generation is accepted to ICLR 2025! 🤖 surpasses human on R2R-VLN for the first time!
                  <!-- <li>(2024-12) &nbsp  <a href="https://arxiv.org/pdf/2412.08467">New preprint</a> on high-quality VLN data generation. 🤖 surpasses human on R2R-VLN for the first time! -->
                  <li>(2024-11) &nbsp  New preprint <a href="https://zunwang1.github.io/DreamRunner">DreamRunner✨</a> for storytelling video generation! My first PhD project at UNC MURGE-Lab🥳!
                  <li>(2024-11) &nbsp  Our VLN survey paper is accepted to TMLR!
                  <li>(2024-08) &nbsp  Started my Ph.D. in the <a href="https://murgelab.cs.unc.edu/">MURGe Lab</a> at UNC Chapel Hill. Hello UNC😆!
                  <li>(2023-10) &nbsp  Attending ICCV2023 @ Paris in person😆! Great pleasure to learn from so many researchers/scholars🥹!
                  <li>(2023-07) &nbsp  One paper accepted to ICCV 2023 as <strong><font color="#FF0000">Oral presentation</font></strong>!
                  <li>(2023-07) &nbsp  I'm awarded a <a href="data/WANG, Zun.pdf">Postgraduate Medal for Academic Excellence</a> from ANU!Photos <a href="images/zun-medal-with-chancellor.JPG">here</a>! </li>
                  <!-- <li>(2022-11) &nbsp  Invited talk at Insight Time of <a href="https://github.com/opendilab">OpenDILab</a> introducing Embodied Navigation! </li> -->
                  <li>(2023-07) &nbsp  I graduated from Masters of Machine Learning and Computer Vision with Commendation from ANU.
                  <li>(2022-11) &nbsp I'm awarded the <a href="data/CM1.pdf">Chancellor's Letter of Commendation</a> from ANU.</li>
                  <li>(2022-09) &nbsp We won <strong>1st place</strong> of the <a href="https://yuankaiqi.github.io/REVERIE_Challenge/">REVERIE VLN Challenge</a> in CSIG 2022!</li>
                  <li>(2022-06) &nbsp We won <strong>1st place</strong> of the <a href="https://ai.google.com/research/rxr/habitat">RxR-Habitat VLN Competition</a> in Embodied AI Workshop, CVPR 2022!</li>
                  <li>(2022-03) &nbsp We have a paper accepted to CVPR 2022!</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>

        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
                <img src='images/dreamrunner2.png' width="160" height="100">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>DreamRunner: Fine-grained Storytelling Video Generation with Retrieval-augmented Motion Adaptation</papertitle>
            <br>
            <strong>Zun Wang</strong>, <e>Jialu Li, Han Lin, Jaehong Yoon, Mohit Bansal</e>
            <br>
          <em>Preprint</em>
          <br>
          <a href="https://arxiv.org/pdf/2411.16657">paper</a> /
          <a href="https://github.com/wz0919/DreamRunner">code</a> /
          <a href="https://zunwang1.github.io/DreamRunner">project page</a>
        </td>
      </tr>
      <tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
                <img src='images/srdf.png' width="160" height="100">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel</papertitle>
            <br>
            <strong>Zun Wang</strong>, <e>Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang</e>
            <br>
          <em>ICLR, 2025</em>
          <br>
          <a href="https://arxiv.org/pdf/2412.08467">paper</a> /
          <a href="https://github.com/wz0919/VLN-SRDF">code</a>
        </td>
      </tr>
      <tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div>
              <img src='images/survey.png' width="160" height="100">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models</papertitle>
        <br>
        <a href="https://www.egr.msu.edu/~zhan1624/">Yue Zhang*</a>, <a href="https://mars-tin.github.io/">Ziqiao Ma*</a>, <a href="https://jialuli-luka.github.io/">Jialu Li<a>*, <a href="https://yanyuanqiao.github.io/">Yanyuan Qiao*</a>, <strong>Zun Wang*</strong> , <a href="https://web.eecs.umich.edu/~chaijy/">Joyce Chai</a>, <a href="http://qi-wu.me/">Qi Wu</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
        <br>
        <em>TMLR, 2024</em>
        <br>
        <a href="https://arxiv.org/abs/2407.07035">paper</a> 
      </td>
    </tr>
    <tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/scalevln.jpg' width="160" height="100">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Scaling Data Generation in Vision-and-Language Navigation</papertitle>
              <br>
              <strong>Zun Wang*</strong>, <e>Jialu Li*, Yicong Hong*, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao</e>
              <br>
              <em>ICCV</em>, 2023, <font color="#FF0000"><strong>Oral presentation (1.9%)</strong></font> </em>
              <br>
              <a href="https://arxiv.org/pdf/2307.15644">paper</a> /
              <a href="https://github.com/wz0919/ScaleVLN">code</a> /
              <a href="https://scalevln.github.io/">project page</a>
            </td>
          </tr>

        </tr>
      </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/traj_1.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation</papertitle>
              <br>
              <a href="https://yiconghong.me/">Yicong Hong*</a>, <strong>Zun Wang*</strong>, <a href="http://qi-wu.me/">Qi Wu</a>, <a href="http://users.cecs.anu.edu.au/~sgould/">Stephen Gould</a>
              <br>
              <em>CVPR</em>, 2022 </em>
              <br>
              <a href="https://arxiv.org/abs/2203.02764">paper</a> /
              <a href="https://github.com/YicongHong/Discrete-Continuous-VLN">code</a>
            </td>
          </tr>

        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/rxr-habitat-winner.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</papertitle>
              <br>
              <e>Dong An*</e>, <strong>Zun Wang*</strong>, <e>Yangguang Li</e>, <e>Yi Wang</e>, <e>Yicong Hong</e>, <e>Yan Huang</e>, <e>Liang Wang</e>, <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>Technical Report </em>, 2022 </em>
              <br>
              <a href="https://arxiv.org/pdf/2206.11610.pdf">paper</a> 
            </td>
          </tr>
        </tr>
    </tbody></table>

        <!-- <details>
          <summary><b>More Publications</b></summary><div>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div>
                      <img src='images/rxr-habitat-winner.jpg' width="160">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models</papertitle>
                    <br>
                    <e>Gengze Zhou</e>, <e>Yicong Hong</e>, <strong>Zun Wang</strong>, <e>Xin Eric Wang</e>, <e>Qi Wu</e>
                    <br>
                    <em>ECCV 2024</em>
                    <br>
                    <a href="https://arxiv.org/pdf/2407.12366">paper</a> 
                  </td>
                </tr>
              </tbody>
            </table>
            

    </div></details>
        

  </br><br>   -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Competitions</heading>
        </td>
      </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div>
                  <img src='images/reverie.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle> REVERIE Challenge @ CSIG 2022 </papertitle>
                <br>
                <e>Our team BPT (<strong>Zun Wang</strong>, Yi Wang, Yinan He, <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Yu Qiao</a>) is the <font color="#FF0000">Winner</font> of both channels (out of 50+ teams).</e>
                <br>
                <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2022-Winner-Tech-Report.pdf">report</a> / 
                <a href="data/赛道1 冠军.pdf">certificate (channel 1)</a> / 
                <a href="data/赛道2冠军.pdf">certificate (channel 2)</a> /
                <a href="https://yuankaiqi.github.io/REVERIE_Challenge/leaderboard.html">leaderboard</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div>
                  <img src='images/rxr-habitat.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle> RxR-Habitat Competition @ CVPR 2022 </papertitle>
                <br>
                <e>Our team Joyboy (Dong An*, <strong>Zun Wang*</strong>, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang,  <a href="https://amandajshao.github.io/">Jing Shao</a>) is the <font color="#FF0000">Winner</font> of the competition. Our solution improves SoTA performance from 37% to 55%.</e>
                <br>
                <a href="https://arxiv.org/pdf/2206.11610.pdf">report</a> / 
                <a href="data/rxr-habitat-cert.pdf">certificate</a> /
                <a href="https://ai.google.com/research/rxr/habitat">leaderboard</a>
              </td>
            </tr>
          </tr>			
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Collaborators</heading>
              <p>
                I work closely and discuss deeply with my friend <a href="https://yiconghong.me/">Dr. Yicong Hong</a>. I also share lots of VLN thinkings and collaborate with my friend <a href="https://marsaki.github.io/">Dong An</a>.
              </p>
            </td>
          </tr>
    

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            <a href= "https://jonbarron.info/">Source pages from here.</a>
          </p>
        </td>
      </tr>
    </tbody></table>
  </td>
</tr>
</table>
</body>

</html>
