<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zun Wang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zun Wang</name>
              </p>
              <p>I'm a first-year CS Ph.D. student at UNC Chapel Hill, advised by <a href="https://www.cs.unc.edu/~mbansal/">Prof. Mohit Bansal</a>. I was a Master of Machine Learning and Computer Vision student at the Australian National University advised by <a href="http://users.cecs.anu.edu.au/~sgould/">Prof. Stephen Gould</a>. In my master time, I also interned at the OpenGVLab, Shanghai AI Laboratory, led by <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Prof. Yu Qiao</a>.
                 Before that, I got my bachelor degree in applied mathematics from the University of Science and Technology of China.
              </p>
              <p style="text-align:center">
                <a href="mailto:zunwang@cs.unc.edu">Email</a> &nbsp/&nbsp
                <a href="data/ZunWang_2024Dec.pdf">CV</a> &nbsp/&nbsp
                <a href="https://twitter.com/ZunWang919">twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=G-jPT9MAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="data/anu-transcript.pdf">Transcript</a> &nbsp/&nbsp -->
                <a href="https://github.com/wz0919/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zunwang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zunwang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <p style="margin-bottom: 5px;">My research goal is to build multimodal, generative, and embodied agents, with current interests in:</p>
                <ul style="margin-top: 0; padding-left: 20px;">
                    <li>Multimodal Understanding and Generation</li>
                    <li>Scalable Learning for Embodied Agents</li>
                    <li>Multimodal Data Generation and Curation</li>
                </ul>
                
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li>(2024-12) &nbsp  New preprint on high-quality VLN data generation. ü§ñ surpasses human on R2R-VLN for the first time!
                  <li>(2024-11) &nbsp  New preprint <a href="https://dreamrunner-story2video.github.io/">DreamRunner‚ú®</a>! My first PhD project at UNC MURGE-Labü•≥!
                  <li>(2024-11) &nbsp  Our VLN survey paper is accepted to TMLR!
                  <li>(2024-08) &nbsp  Started my Ph.D. in the <a href="https://murgelab.cs.unc.edu/">MURGe Lab</a> at UNC Chapel Hill. Hello UNCüòÜ!
                  <li>(2023-10) &nbsp  Attending ICCV2023 @ Paris in personüòÜ! Great pleasure to learn from so many researchers/scholarsü•π!
                  <li>(2023-07) &nbsp  One paper accepted to ICCV 2023 as <strong><font color="#FF0000">Oral presentation</font></strong>!
                  <li>(2023-07) &nbsp  I'm awarded a <a href="data/WANG, Zun.pdf">Postgraduate Medal for Academic Excellence</a> from ANU!Photos <a href="images/zun-medal-with-chancellor.JPG">here</a>! </li>
                  <!-- <li>(2022-11) &nbsp  Invited talk at Insight Time of <a href="https://github.com/opendilab">OpenDILab</a> introducing Embodied Navigation! </li> -->
                  <li>(2023-07) &nbsp  I graduated from Masters of Machine Learning and Computer Vision with Commendation from ANU.
                  <li>(2022-11) &nbsp I'm awarded the <a href="data/CM1.pdf">Chancellor's Letter of Commendation</a> from ANU.</li>
                  <li>(2022-09) &nbsp We won <strong>1st place</strong> of the <a href="https://yuankaiqi.github.io/REVERIE_Challenge/">REVERIE VLN Challenge</a> in CSIG 2022!</li>
                  <li>(2022-06) &nbsp We won <strong>1st place</strong> of the <a href="https://ai.google.com/research/rxr/habitat">RxR-Habitat VLN Competition</a> in Embodied AI Workshop, CVPR 2022!</li>
                  <li>(2022-03) &nbsp We have a paper accepted to CVPR 2022!</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>

        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
                <img src='images/dreamrunner2.png' width="160" height="100">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>DreamRunner: Fine-grained Storytelling Video Generation with Retrieval-augmented Motion Adaptation</papertitle>
            <br>
            <strong>Zun Wang</strong>, <e>Jialu Li, Han Lin, Jaehong Yoon, Mohit Bansal</e>
            <br>
          <em>Preprint</em>
          <br>
          <a href="https://arxiv.org/pdf/2411.16657">paper</a> /
          <a href="https://github.com/wz0919/DreamRunner">code</a> /
          <a href="https://dreamrunner-story2video.github.io/">project page</a>
        </td>
      </tr>
      <tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
                <img src='images/srdf.png' width="160" height="100">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel</papertitle>
            <br>
            <strong>Zun Wang</strong>, <e>Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang</e>
            <br>
          <em>Preprint</em>
          <br>
          <a href="data/Surpass_Human_VLN_Arxiv.pdf">paper</a>
        </td>
      </tr>
      <tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div>
              <img src='images/survey.png' width="160" height="100">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models</papertitle>
        <br>
        <a href="https://www.egr.msu.edu/~zhan1624/">Yue Zhang*</a>, <a href="https://mars-tin.github.io/">Ziqiao Ma*</a>, <a href="https://jialuli-luka.github.io/">Jialu Li<a>*, <a href="https://yanyuanqiao.github.io/">Yanyuan Qiao*</a>, <strong>Zun Wang*</strong> , <a href="https://web.eecs.umich.edu/~chaijy/">Joyce Chai</a>, <a href="http://qi-wu.me/">Qi Wu</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
        <br>
        <em>TMLR, 2024</em>
        <br>
        <a href="https://arxiv.org/abs/2407.07035">paper</a> 
      </td>
    </tr>
    <tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/scalevln.jpg' width="160" height="100">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Scaling Data Generation in Vision-and-Language Navigation</papertitle>
              <br>
              <strong>Zun Wang*</strong>, <e>Jialu Li*, Yicong Hong*, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao</e>
              <br>
              <em>ICCV</em>, 2023, <font color="#FF0000"><strong>Oral presentation (1.9%)</strong></font> </em>
              <br>
              <a href="">paper</a> /
              <a href="https://github.com/wz0919/ScaleVLN">code</a> /
              <a href="https://scalevln.github.io/">project page</a>
            </td>
          </tr>

        </tr>
      </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/traj_1.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation</papertitle>
              <br>
              <a href="https://yiconghong.me/">Yicong Hong*</a>, <strong>Zun Wang*</strong>, <a href="http://qi-wu.me/">Qi Wu</a>, <a href="http://users.cecs.anu.edu.au/~sgould/">Stephen Gould</a>
              <br>
              <em>CVPR</em>, 2022 </em>
              <br>
              <a href="https://arxiv.org/abs/2203.02764">paper</a> /
              <a href="https://github.com/YicongHong/Discrete-Continuous-VLN">code</a>
            </td>
          </tr>

        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div>
                    <img src='images/rxr-habitat-winner.jpg' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</papertitle>
                  <br>
                  <e>Dong An*</e>, <strong>Zun Wang*</strong>, <e>Yangguang Li</e>, <e>Yi Wang</e>, <e>Yicong Hong</e>, <e>Yan Huang</e>, <e>Liang Wang</e>, <a href="https://amandajshao.github.io/">Jing Shao</a>
                  <br>
                  <em>Technical Report </em>, 2022 </em>
                  <br>
                  <a href="https://arxiv.org/pdf/2206.11610.pdf">paper</a> 
                </td>
              </tr>
            </tr>
        </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Competitions</heading>
        </td>
      </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div>
                  <img src='images/reverie.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle> REVERIE Challenge @ CSIG 2022 </papertitle>
                <br>
                <e>Our team BPT (<strong>Zun Wang</strong>, Yi Wang, Yinan He, <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Yu Qiao</a>) is the <font color="#FF0000">Winner</font> of both channels (out of 50+ teams).</e>
                <br>
                <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2022-Winner-Tech-Report.pdf">report</a> / 
                <a href="data/ËµõÈÅì1 ÂÜ†ÂÜõ.pdf">certificate (channel 1)</a> / 
                <a href="data/ËµõÈÅì2ÂÜ†ÂÜõ.pdf">certificate (channel 2)</a> /
                <a href="https://yuankaiqi.github.io/REVERIE_Challenge/leaderboard.html">leaderboard</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div>
                  <img src='images/rxr-habitat.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle> RxR-Habitat Competition @ CVPR 2022 </papertitle>
                <br>
                <e>Our team Joyboy (Dong An*, <strong>Zun Wang*</strong>, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang,  <a href="https://amandajshao.github.io/">Jing Shao</a>) is the <font color="#FF0000">Winner</font> of the competition. Our solution improves SoTA performance from 37% to 55%.</e>
                <br>
                <a href="https://arxiv.org/pdf/2206.11610.pdf">report</a> / 
                <a href="data/rxr-habitat-cert.pdf">certificate</a> /
                <a href="https://ai.google.com/research/rxr/habitat">leaderboard</a>
              </td>
            </tr>
          </tr>			
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Collaborators</heading>
              <p>
                I work closely and discuss deeply with my friend <a href="https://yiconghong.me/">Dr. Yicong Hong</a>. I also share lots of VLN thinkings and collaborate with my friend <a href="https://marsaki.github.io/">Dong An</a>.
              </p>
            </td>
          </tr>
    

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            <a href= "https://jonbarron.info/">Source pages from here.</a>
          </p>
        </td>
      </tr>
    </tbody></table>
  </td>
</tr>
</table>
</body>

</html>
