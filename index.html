<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zun Wang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zun Wang</name>
              </p>
              <p>I am a Master of Machine Learning and Computer Vision student at the Australia National University. I finished my master research project under the supervision of <a href="http://users.cecs.anu.edu.au/~sgould/">Prof. Stephen Gould</a>. Before that, I got my bachelor degree in applied mathematics from the University of Science and Technology of China.
              <p>
                Currently I'm doing a research internship at the videoINTERN Group, Shanghai AI Laboratory, led by <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Prof. Yu Qiao</a>.
              <p> I'll be graduted from ANU in mid-2023 and I'm actively looking for PhD oppotunities in mid-2023, too.
              </p>
              <p style="text-align:center">
                <a href="mailto:zun.wang@anu.edu.au">Email</a> &nbsp/&nbsp
                <a href="data/onepagecv.pdf">CV</a> &nbsp/&nbsp
                <a href="data/anu-transcript.pdf">Transcript</a> &nbsp/&nbsp
                <a href="https://github.com/wz0919/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zunwang.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/zunwang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have broad research interests in Computer Vision, Natural Language Processing and Robotics. Currently my main research focuses on <b>Embodied AI</b>, and in particular <b>Vision-and-Language Navigation</b>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li>(2022-09) &nbsp We won <strong>1st place</strong> of the <a href="https://yuankaiqi.github.io/REVERIE_Challenge/">REVERIE VLN Challenge</a> in CSIG 2022!</li>
                  <li>(2022-06) &nbsp We won <strong>1st place</strong> of the <a href="https://ai.google.com/research/rxr/habitat">RxR-Habitat VLN Competition</a> in Embodied AI Workshop, CVPR 2022!</li>
                  <li>(2022-03) &nbsp We have a paper accepted to CVPR 2022!</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
        </tr>
      </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/traj_1.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation</papertitle>
              <br>
              <a href="https://yiconghong.me/">Yicong Hong*</a>, <strong>Zun Wang*</strong>, <a href="http://qi-wu.me/">Qi Wu</a>, <a href="http://users.cecs.anu.edu.au/~sgould/">Stephen Gould</a>.
              <br>
              <em>CVPR</em>, 2022 </em>
              <br>
              <a href="https://arxiv.org/abs/2203.02764">paper</a> /
              <a href="https://github.com/YicongHong/Discrete-Continuous-VLN">code</a> /
              <a href="https://github.com/YicongHong/Discrete-Continuous-VLN">bib</a>
            </td>
          </tr>

        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div>
                    <img src='images/rxr-habitat-winner.jpg' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</papertitle>
                  <br>
                  <e>Dong An*</e>, <strong>Zun Wang*</strong>, <e>Yangguang Li</e>, <e>Yi Wang</e>, <e>Yicong Hong</e>, <e>Yan Huang</e>, <e>Liang Wang</e>, <a href="https://amandajshao.github.io/">Jing Shao</a>.
                  <br>
                  <em>Technical Report </em>, 2022 </em>
                  <br>
                  <a href="https://arxiv.org/pdf/2206.11610.pdf">paper</a> 
                </td>
              </tr>
            </tr>
        </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Competitions</heading>
        </td>
      </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div>
                  <img src='images/reverie.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle> 3rd REVERIE Challenge @ CSIG 2022 </papertitle>
                <br>
                <e>Our team BPT (<strong>Zun Wang</strong>, Yi Wang, Yinan He, <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Yu Qiao</a>) is the <font color="#FF0000">Winner</font> of both channels (out of 50+ teams).</e>
                <br>
                <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2022-Winner-Tech-Report.pdf">report</a> / 
                <a href="data/ËµõÈÅì1 ÂÜ†ÂÜõ.pdf">certificate (channel 1)</a> / 
                <a href="data/ËµõÈÅì2ÂÜ†ÂÜõ.pdf">certificate (channel 2)</a> /
                <a href="https://yuankaiqi.github.io/REVERIE_Challenge/leaderboard.html">leaderboard</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div>
                  <img src='images/rxr-habitat.gif' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle> 2nd RxR-Habitat Competition @ CVPR 2022 </papertitle>
                <br>
                <e>Our team Joyboy (Dong An*, <strong>Zun Wang*</strong>, Yangguang Li, Yi Wang, Yicong Hong, Yan Huang, Liang Wang,  <a href="https://amandajshao.github.io/">Jing Shao</a>) is the <font color="#FF0000">Winner</font> of the competition. Our solution improves SoTA performance from 37% to 55%.</e>
                <br>
                <a href="https://arxiv.org/pdf/2206.11610.pdf">report</a> / 
                <a href="data/rxr-habitat-cert.pdf">certificate</a> /
                <a href="https://ai.google.com/research/rxr/habitat">leaderboard</a>
              </td>
            </tr>
          </tr>			
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            <a href= "https://jonbarron.info/">Source pages from here.</a>
          </p>
        </td>
      </tr>
    </tbody></table>
  </td>
</tr>
</table>
</body>

</html>
